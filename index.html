<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/html">
<head>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/>
    <meta name="description" content=""/>
    <meta name="author" content=""/>
    <title>Michal Stefanik</title>
    <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico"/>
    <!-- Font Awesome icons (free version)-->
    <script src="https://use.fontawesome.com/releases/v6.1.0/js/all.js" crossorigin="anonymous"></script>
    <!-- Google fonts-->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet"
          type="text/css"/>
    <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css"/>
    <!-- Core theme CSS (includes Bootstrap)-->
    <link href="css/styles.css" rel="stylesheet"/>
</head>
<body id="page-top">
<!-- Navigation-->
<nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
    <a class="navbar-brand js-scroll-trigger" href="#page-top">
        <span class="d-block d-lg-none">Michal ≈†tef√°nik</span>
        <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2"
                                             src="assets/img/profile.jpg" alt="..."/></span>
    </a>
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive"
            aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span
            class="navbar-toggler-icon"></span></button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav">
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#publications">Publications</a></li>
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#experience">Experience</a></li>
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#talks">Talks</a></li>
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#other">Other</a></li>
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#afk">Disconnected</a></li>
            <!--                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#awards">Awards</a></li>-->
        </ul>
    </div>
</nav>
<!-- Page Content-->
<div class="container-fluid p-0">
    <!-- About-->
    <section class="resume-section" id="about">
        <div class="resume-section-content">
            <h1 class="mb-0">
                <!--                        <span class="text-primary">Michal ≈†tef√°nik</span>-->
                Michal ≈†tef√°nik
            </h1>
            <div class="subheading mb-5">
                Researcher in Natural Language Processing
                <p><a href="mailto:stefanik.m@mail.muni.cz"> stefanik.m@mail.muni.cz</a></p>
            </div>
            <div class="social-icons">
                <a class="social-icon" href="https://scholar.google.com/citations?user=9p-110IAAAAJ&hl=en&oi=ao"><img
                        width="48%" height="48%" object-fit="contain" src="assets/img/gscholar-icon.png"/></a>
                <a class="social-icon" href="https://www.linkedin.com/in/stefanikm"><i
                        class="fab fa-linkedin-in"></i></a>
                <a class="social-icon" href="https://github.com/stefanik12"><i class="fab fa-github"></i></a>
                <a class="social-icon" href="https://www.facebook.com/stefanik.mich"><i
                        class="fab fa-facebook-f"></i></a>
                <p></p>
                <p></p>
            </div>
            <p class="lead mb-5"></p>
            <p class="lead mb-6">Welcome to my webpage! I am a third-year PhD candidate in the <a
                    href="https://www.fi.muni.cz/about/">Faculty of Informatics</a> of
                <a href=https://www.muni.cz/en>Masaryk University</a>
                and an NLP team lead in <a href="https://www.gaussalgo.com/en/">Gauss Algorithmic</a>.
            </p>
            <p class="lead mb-6">
                My <strong>research interests</strong> are in everything around <strong>robustness of the language
                models</strong>.
                Among others, this closely relates to <strong>domain adaptation</strong>,
                <strong>generalization</strong>, <strong>quality estimation</strong> or robust
                <strong>low-resource</strong> applications.

            </p>
            <p class="lead mb-6">
                Apart from my own research, I am a founder of student <a href="#supervisor">Transformers Club</a>, whose
                alumni were awarded with
                two Dean's awards, a SVOC prize or a 1st place in NAACL DADC workshop track.
            </p>
            <p class="lead mb-6">
                Over the last five years, I've also contributed and led the delivery of the <strong>industrial
                applications</strong> of the most recent NLP research
                in multilingual <a href="https://www.gaussalgo.com/en/case-studies/lectura-and-seo-text-optimalization">language
                generation</a>, or
                <a href="https://www.gaussalgo.com/en/cyber-security/revealing-sensitive-documents-with-ner-practical-case-study">entity
                    recognition</a>,
                all the way from the research ideas to the scalable, containerized deployments, now daily serving the
                fascinating NLP technologies to end users.
            </p>
        </div>
    </section>
    <hr class="m-0"/>
    <!-- Experience-->
    <section class="resume-section" id="publications">
        <div class="resume-section-content">
            <h2 class="mb-5">Publications</h2>
            <p>Below you can find my most recent (and exciting) publications
            </p>
            </br>
            <h3 class="mb-2"><span class="text-primary">2023</span></h3>

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Concept-aware Training Improves In-context Learning Ability of Language Models</h3>
                    <!--                    <div class="subheading mb-3"><span class="text-primary">Michal ≈†tef√°nik</span>, Marek Kadlƒç√≠k</div>-->
                    <div class="lead mb-3">Under review, pre-print <a
                            href="https://openreview.net/pdf?id=WF0U9Dta8gT">available
                        here</a>.
                    </div>
                    <p>Previous work curating in-context learning models assumes that ICL emerges from vast
                        over-parametrization or
                        the scale of multitask training. However, recent theoretical work
                        attributes ICL emergence to specific properties of training data and creates functional
                        in-context learners in small-scale, synthetic settings.

                        Inspired by these findings, we propose a Concept-aware Training (CoAT) method
                        constructing training scenarios such that it is beneficial for the LM to capture the
                        analogical reasoning concepts. We measure that data sampling of CoAT
                        consistently improves models' ICL on unseen reasoning tasks, making the in-context
                        learners trained with CoAT on only two QA datasets to perform
                        comparably to models trained on over 1600 tasks.
                        Our analyses attribute some of CoAT's empirical improvements to an enhanced ability to
                        benefit from natural concepts in demonstrations and to mitigation of models' over-reliance on
                        their learned semantic priors.
                    </p>
                </div>
            </div>

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Calc-X: Enriching Arithmetical Chain-of-Thoughts Datasets by Interaction with
                        Symbolic Systems</h3>
                    <div class="subheading mb-3">Marek Kadlƒç√≠k*, <span class="text-primary">Michal ≈†tef√°nik</span>*, Ond≈ôej Sotol√°≈ô, Vlastimil Martinek (*equal)</div>
                    <div class="lead mb-3">In <a href="https://drive.google.com/file/d/18XwPXO0dgudX8FiAMAo60w5n-0PUsADa/view?usp=sharing">
                        Proceedings of EMNLP 2023: Main track</a> (to appear).
                    </div>
                    <p>Language models are notoriously
                        inclined to make factual errors in tasks requiring arithmetic reasoning.
                        To enable language models to circumvent this deficiency and offload critical computation to a
                        symbolic system, we create a collection of Calc-X datasets that demonstrate the appropriate use
                        of a calculator in reasoning chains.
                        We survey and unify several existing chain-of-thought datasets into a proposed novel format,
                        resulting in a standard collection of over 300,000 samples requiring arithmetic reasoning.
                        Finally, we use the new collection to train open-source calculator-assisted language models and
                        show that models trained on Calc-X almost double the accuracy of generating correct results
                        compared to baselines.
                        We make all Calc-X datasets and models <a href="https://huggingface.co/emnlp2023">publicly available</a>.
                    </p>
                </div>
            </div>


            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Can In-context Learners Learn a Reasoning Concept from Demonstrations?</h3>
                    <div class="subheading mb-3"><span class="text-primary">Michal ≈†tef√°nik</span>, Marek Kadlƒç√≠k</div>
                    <div class="lead mb-3">In <a href="https://arxiv.org/abs/2212.01692">Proceedings of ACL 2023 Natural Language Reasoning (NLRSE) workshop</a> ¬∑ ü•á Best paper award</div>
                    <p>Recent work analysing the functioning of in-context learners show that instead of learning new associations from the input,
                        models largely rely on their pre-trained knowledge, such as the sentiment of the labels.
                        We argue that the in-context learning evaluations using a random demonstrations can not disentangle models' reliance on such features,
                        as random demonstrations rarely present functional relations useful for prediction.
                        Hence, we propose to evaluate in-context learners with demonstrations sharing with predicted sample a specific, informative reasoning concept, which we extract from human explanations.
                        We find that most of the recent in-context learners can not consistently benefit from the demonstrated concepts, irrespective of the model size.
                        However, some models are more sensitive to concepts than others, such as T0 models, which can benefit from concepts in 7 of 8 evaluation scenarios.
                    </p>
                </div>
            </div>

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Soft Alignment Objectives for Robust Adaptation of Language Generation</h3>
                    <div class="subheading mb-3"><span class="text-primary">Michal ≈†tef√°nik</span>, Marek Kadlƒç√≠k, Petr
                        Sojka
                    </div>
                    <div class="lead mb-3">In <a href="https://aclanthology.org/2023.acl-long.492">Proceedings of the 61th Annual
                        Meeting of the ACL (ACL 2023): Main track</a></div>
                    <p>The traditional adaptation by continuous in-domain training weakens the model's ability to
                        generalize to other domains, making the open-ended deployment of these models prone to errors.
                        This work introduces two novel objectives, grounded in a semantic similarity of the generating
                        hypothesis to the reference. We show that (1) grounding of the training signal in semantic
                        similarity can mitigate the catastrophic forgetting of domain adaptation, while (2) in many
                        cases improving the performance on the adapted domain, (3) with negligible additions to compute
                        costs. In the broader sense, our objectives grounded in a soft token-level alignment pioneer the
                        exploration of the middle ground between the efficient but narrow exact-match token-level
                        objectives and expressive but computationally- and resource-intensive sentence-level
                        objectives.</p>
                        <p><a href="#talks">Conference talk ‚¨áÔ∏è</a>Ô∏è</p>
                </div>
            </div>


            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of
                        Question Answering Models</h3>
                    <div class="subheading mb-3">Luk√°≈° Mikula*, <span class="text-primary">Michal ≈†tef√°nik</span>*, Marek Petroviƒç, Petr Sojka (*equal)
                    </div>
                    <div class="lead mb-3">Accepted through ARR, to appear at a *ACL conference; <a
                            href="https://arxiv.org/abs/2305.06841">pre-print</a></div>
                    <p>We propose a framework for measuring a scale of models' reliance on any identified spurious
                        feature and measure the size of such reliance for some previously-reported features while
                        uncovering several new ones. We assess the robustness towards a large set of known and new-found
                        prediction biases for a variety of pre-trained models and state-of-the-art debiasing methods in
                        Question Answering (QA) and compare it to a resampling baseline. We find that (i) the observed
                        OOD gains of debiasing methods can not be explained by mitigation or enlargement of the
                        addressed bias and subsequently evaluate that (ii) the biases are vastly shared among QA
                        datasets. Our findings motivate future work to refine the reports of LLMs' robustness to a level
                        of specific spurious correlations.
                    </p>
                </div>
            </div>

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Resources and Few-shot Learners for In-context Learning in Slavic Languages</h3>
                    <div class="subheading mb-3"><span class="text-primary">Michal ≈†tef√°nik</span>, Marek Kadlƒç√≠k, Piotr
                        Gramacki, Petr Sojka
                    </div>
                    <div class="lead mb-3">In <a href="https://aclanthology.org/2023.bsnlp-1.12/">Proceedings of EACL
                        SlavicNLP 2023 workshop</a> ¬∑ ü•á Best paper award
                    </div>
                    <p>In this work, we collect the infrastructure necessary for training and evaluation of ICL
                        in a selection of Slavic languages: Czech, Polish, and Russian. We link a diverse set of
                        datasets and cast these into a unified instructional format through a set of transformations
                        and newly-crafted templates written purely in target languages.Using the newly-curated dataset,
                        we evaluate a set of the most recent in-context learners and compare their results to the
                        supervised baselines. Finally, we train, evaluate and publish a set of in-context learning
                        models.
                        We find that the massive multitask training can be outperformed by single-task training in the
                        target language, uncovering the potential for specializing in-context learners to the
                        language(s) of their application.

                    </p>
                </div>
            </div>


            <h3 class="mb-2"><span class="text-primary">2022</span></h3>
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Methods for Estimating and Improving Robustness of Language Models</h3>
                    <div class="subheading mb-3"><span class="text-primary">Michal ≈†tef√°nik</span></div>
                    <div class="lead mb-3">In <a href="https://aclanthology.org/2022.naacl-srw.6/">Proceedings of NAACL
                        2022: SRW workshop</a></div>
                    <p>Despite their outstanding performance, large language models (LLMs) suffer notorious flaws
                        related to their preference for simple, surface-level textual relations over full semantic
                        complexity of the problem. This proposal investigates a common denominator of this problem in
                        their weak ability to generalise outside of the training domain. We survey diverse research
                        directions providing estimations of model generalisation ability and find that incorporating
                        some of these measures in the training objectives leads to enhanced distributional robustness of
                        neural models. Based on these findings, we present future research directions towards enhancing
                        the robustness of LLMs.</p>
                    <p><a href="#talks">Conference talk ‚¨áÔ∏è</a>Ô∏è</p>
                </div>
            </div>
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Adaptor: Objective-Centric Adaptation Library for Language Models</h3>
                    <div class="subheading mb-3"><span class="text-primary">Michal ≈†tef√°nik</span>, V√≠t Novotn√Ω, Nikola
                        Groverov√°, Petr Sojka
                    </div>
                    <div class="lead mb-3">In <a href="https://aclanthology.org/2022.acl-demo.26/">Proceedings of the
                        60th Annual Meeting of the ACL: Demonstrations</a></div>
                    <p>This paper introduces Adaptor library, transposing the traditional model-centric approach
                        composed of pre-training + fine-tuning steps to objective-centric approach, composing the
                        training from applications of selected objectives. We survey research directions that can
                        benefit from enhanced objective-centric experimentation in multitask training, custom objectives
                        development, dynamic training curricula, or domain adaptation and demonstrate the practical
                        applicability of Adaptor in selected unsupervised domain adaptation scenarios.</p>
                    <p><a href="#talks">Introduction talk ‚¨áÔ∏è</a>Ô∏è</p>
                </div>
            </div>
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">When FastText Pays Attention: Efficient Estimation of Word Representations using
                        Constrained Positional Weighting</h3>
                    <div class="subheading mb-3">V√≠t Novotn√Ω, <span class="text-primary">Michal ≈†tef√°nik</span>, Eniafe
                        Festus Ayetiran, Petr Sojka, Radim ≈òeh≈Ø≈ôek
                    </div>
                    <div class="lead mb-3">In <a href="https://lib.jucs.org/article/69619/">JUCS: The Journal of
                        Universal Computer Science</a></div>
                    <p>We propose a constrained positional model, which adapts the sparse attention mechanism from
                        neural machine translation to improve the speed of the positional model. Our constrained model
                        outperforms the positional model on language modelling and trains twice as fast.</p>
                </div>
            </div>

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Applications of deep language models for reflective writings</h3>
                    <div class="subheading mb-3">Jan Nehyba*, <span class="text-primary">Michal ≈†tef√°nik</span>* (*equal)
                    </div>
                    <div class="lead mb-3">In <a href="https://rdcu.be/cUWGY">Education and Information Technologies</a>
                    </div>
                    <p>Social sciences expose many cognitively complex, highly qualified, or fuzzy
                        problems, whose resolution relies primarily on expert judgement rather than
                        automated systems. One of such instances that we study in this work is a
                        reflection analysis in the writings of student teachers. We share a hands-on
                        experience on how these challenges can be successfully tackled in data collection
                        for machine learning.</p>
                </div>
            </div>

            <h3 class="mb-2"><span class="text-primary">2021</span></h3>

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">RegEMT: Regressive Ensemble for Machine Translation Quality Evaluation</h3>
                    <div class="subheading mb-3"><span class="text-primary">Michal ≈†tef√°nik</span>, V√≠t Novotn√Ω, Petr
                        Sojka
                    </div>
                    <div class="lead mb-3">In <a href="https://aclanthology.org/2021.wmt-1.112/">Proceedings of the
                        Sixth Conference on Machine Translation (WMT)</a></div>
                    <p>This work introduces a simple regressive ensemble for evaluating machine translation quality
                        based on a set of novel and established metrics. We evaluate the ensemble using a correlation to
                        expert-based MQM scores of the WMT 2021 Metrics workshop. In both monolingual and zero-shot
                        cross-lingual settings, we show a significant performance improvement over single metrics. In
                        the cross-lingual settings, we also demonstrate that an ensemble approach is well-applicable to
                        unseen languages. Furthermore, we identify a strong reference-free baseline that consistently
                        outperforms the commonly-used BLEU and METEOR measures and significantly improves our ensemble‚Äôs
                        performance.</p>
                </div>
            </div>

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">One Size Does Not Fit All: Finding the Optimal Subword Sizes for FastText Models
                        across Languages</h3>
                    <div class="subheading mb-3">V√≠t Novotn√Ω, Eniafe Festus Ayetiran, Dalibor Baƒçovsk√Ω, D√°vid Lupt√°k,
                        <span class="text-primary">Michal ≈†tef√°nik</span>, Petr Sojka
                    </div>
                    <div class="lead mb-3">In <a href="https://aclanthology.org/2021.ranlp-1.120/">Proceedings of the
                        International Conference on Recent Advances in NLP (RANLP 2021)</a></div>
                    <p>In our work, we find the optimal subword sizes on the English, German, Czech, Italian, Spanish,
                        French, Hindi, Turkish, and Russian word analogy tasks. We then propose a simple n-gram coverage
                        model and we show that it predicts better-than-default subword sizes on the Spanish, French,
                        Hindi, Turkish, and Russian word analogy tasks. We show that the optimization of fastText‚Äôs
                        subword sizes matters and results in a 14% improvement on the Czech word analogy task. We also
                        show that expensive parameter optimization can be replaced by a simple n-gram coverage model
                        that consistently improves the accuracy of fastText models on the word analogy tasks by up to 3%
                        compared to the default subword sizes, and that it is within 1% accuracy of the optimal subword
                        sizes.</p>
                </div>
            </div>
        </div>
    </section>
    <hr class="m-0"/>
    <!-- Experience-->
    <section class="resume-section" id="experience">
        <div class="resume-section-content">
            <h2 class="mb-5">Experience</h2>
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">NLP Scientist - Team Lead</h3>
                    <div class="subheading mb-3">Gauss Algorithmic</div>
                    <div>Guiding the most recent NLP research applications from whiteboards to the users.
                    </div>
                    <br>Creating the ideas that widen the applicability of NLP to novel areas but also coordinating the
                    delivery within the team of three tenure NLP scientists and multiple software developers, operations
                    engineers, business developers and copywriters. </br>
                    <br>See our <a href="https://www.gaussalgo.com/insights">blogs</a>, <a
                        href="https://www.gaussalgo.com/case-studies/seo-text-proofreading-and-optimization">case
                    studies</a>, <a href="https://demo-hub.gaussalgo.com/">demos</a> or <a
                        href="https://github.com/gaussalgo/adaptor">open-source projects</a>.</br>
                </div>
                <div class="flex-shrink-0"><span class="text-primary">March 2021 - now</span></div>
            </div>


            <div class="d-flex flex-column flex-md-row justify-content-between">

                <div class="flex-grow-1">
                    <h3 class="mb-0">Technical lead</h3>
                    <div class="subheading mb-3">Intelligent Back Office: Grant Project</div>
                    <p>Coordination of the research team of 5-6 researchers within a larger, multi-organization
                        <a href="https://www.muni.cz/en/research/projects/64989">grant project</a> focused on improving
                        document processing quality for specialised domain(s).</p>
                    <div>Our research objectives are (a) to enhance the quality of domain-specific OCR text extraction
                        utilising semi-supervised language modelling and (b) to utilise relative positional information
                        of the document segments for a more accurate extraction of the named entities.
                    </div>
                    <br></br>
                </div>
                <div class="flex-shrink-0"><span class="text-primary">January 2022 - May 2023</span></div>

            </div>


            <div class="d-flex flex-column flex-md-row justify-content-between">
                <div class="flex-grow-1">
                    <h3 class="mb-0">NLP Scientist</h3>
                    <div class="subheading mb-3">Gauss Algorithmic</div>
                    <div>Creating the prototypes of NLP applications for specific use-cases,
                        in multilingual classification, named entity recognition, or language generation.
                    </div>
                    <br>Responsibilities ranging from communicating the customers' expectations to stable deployments of
                    containerized applications.</br>
                    <br>Implementations using Python & PyTorch, deployments using CI, Docker & Kubernetes in AWS and
                    Google Cloud.</br>
                    <br></br>
                </div>
                <div class="flex-shrink-0"><span class="text-primary">June 2018 - March 2021</span></div>
            </div>

            <div class="d-flex flex-column flex-md-row justify-content-between">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Junior Software Developer</h3>
                    <div class="subheading mb-3">Red Hat Software</div>
                    <div>Enhancing the company search engines with semantic text representations and classification,
                        within the <a href="https://github.com/searchisko/project-classifier-poc">Searchisko</a> & other
                        open-source projects.
                    </div>
                    <br>Application of NLP technologies such as Word2Vec, and FastText. Implementations in Python &
                    Java, deployments in OpenShift.
                    </br>
                    <br></br>
                </div>
                <div class="flex-shrink-0"><span class="text-primary">March 2015 - June 2018</span></div>
            </div>
            <div class="d-flex flex-column flex-md-row justify-content-between">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Stream Processing Developer</h3>
                    <div class="subheading mb-3">CSIRT: Cyber Security Team of Masaryk Univerity</div>
                    <div>Research & development of intrusion detection methods in scalable, streaming paradigms.</div>
                    <br>Implementations of unsupervised and semi-supervised methods, Apache Spark & Python.
                    Open-sourced in <a href="https://github.com/CSIRT-MU/Stream4Flow">Stream4Flow</a> project.
                    </br>

                </div>
                <div class="flex-shrink-0"><span class="text-primary">March 2016 - September 2017</span></div>
            </div>
        </div>
    </section>
    <hr class="m-0"/>
    <!-- Talks -->
    <section class="resume-section" id="talks">
        <div class="resume-section-content">
            <h2 class="mb-5">Talks</h2>


            <h3 class="mb-0">Soft Alignment Objectives for Robust Adaptation of Language Generation</h3>
            <div class="subheading mb-3"><span class="text-primary">ACL 2023 conference in Toronto, Canada, July 2023</span>
                <br></br>
                <p>A 6 min exposition of our ACL paper proposing more robust training objectives for language generation.</p>
                <iframe width="560" height="315" src="https://www.youtube.com/embed/YWHGuichMCA"
                        title="YouTube video player" frameborder="0"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                        allowfullscreen></iframe>
                <br></br>
            </div>

            <h3 class="mb-0">Learning to Learn: Hands-on Tutorial in Using and Improving Few-shot Language Models</h3>
            <div class="subheading mb-3"><span class="text-primary">Workshop on Machine Learning Prague conference, in Prague, Czechia, June 2023</span>
                <br></br>
                <p>Organisation of the workshop covering most recent topics with hands-on tutorials in in-context learning.<br>
                    <a href="https://github.com/gaussalgo/L2L_MLPrague23">Workshop github</a> &
                    <a href="https://github.com/gaussalgo/L2L_MLPrague23/blob/main/presentation.pdf">Presentation slides</a> with links to the literature.</p>
                <img alt="MLMU Cover Photo"
                     src="assets/img/mlprague.jpeg" loading="eager"
                     class="rounded-full object-cover" style="width: 560px; height: 315px;">
                <br></br>
            </div>


            <h3 class="mb-0">Resources and Few-shot Learners for In-context Learning in Slavic Languages</h3>
            <div class="subheading mb-3"><span class="text-primary">EACL 2023: Slavic NLP workshop in Dubrovnik, Croatia, May 2023 ¬∑ ü•á Best paper award</span>
                <br></br>
                <p>A 10 min overview of our work in building and evaluating in-context learning in Slavic languages.</p>
                <iframe width="560" height="315" src="https://www.youtube.com/embed/Sun2FhQTQZI"
                        title="YouTube video player" frameborder="0"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                        allowfullscreen></iframe>
                <br></br>
            </div>

            <h3 class="mb-0">Learning to Learn: Training Language Models to Understand Tasks from Few Examples</h3>
            <div class="subheading mb-3"><span class="text-primary">Community talk: Machine Learning MeetUp Brno, October 2022 (Offline)</span>
                <br></br>
                <p>Informal presentation of our methodology and trained models for Few-shot in Czech and other
                    Non-English languages.<br>
                    <a href="https://michal-stefanik.medium.com/few-shot-learning-goes-multilingual-9837b2962901">Medium
                        Blog</a> &
                    <a href="https://www.facebook.com/events/6396221753725366/?active_tab=about">Event</a> &
                    <a href="https://drive.google.com/file/d/1xO7V0AmE57eH4FAoH517XZPQ0_DtOoby/view?usp=sharing">Presentation
                        with links</a> &
                    <a href="https://huggingface.co/gaussalgo/mt5-large-priming-QA_en-cs">HuggingFace models</a>.</p>
                <img alt="MLMU Cover Photo"
                     src="assets/img/mlmu.jpeg" loading="eager"
                     class="rounded-full object-cover" style="width: 560px; height: 315px;">
                <br></br>
            </div>


            <h3 class="mb-0">Mitigating Biases of QA Models by Simple Resampling Methods</h3>
            <div class="subheading mb-3"><span class="text-primary">2022 Annual Conference of the NAACL: DADC Workshop, July 2022</span>
                <br></br>
                <p>Results announcement and recording of Supersamplers' team presentation, live on NAACL 2022
                    DADC workshop in Seattle.</p>
                <iframe width="560" height="315" src="https://www.youtube.com/embed/lMochKhgdZg"
                        title="YouTube video player" frameborder="0"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                        allowfullscreen></iframe>
                <br></br>
            </div>


            <h3 class="mb-0">Methods for Estimating and Improving Robustness of Language Models</h3>
            <div class="subheading mb-3"><span
                    class="text-primary">NAACL 2022: SRW workshop, July 2022</span>
                <br></br>
                <p>A 5min talk covering my thesis proposal presented on NAACL 2022 SRW in Seattle.</p>
                <iframe width="560" height="315" src="https://www.youtube.com/embed/dTltkOXeeyk"
                        title="YouTube video player" frameborder="0"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                        allowfullscreen></iframe>
                <br></br>
            </div>

            <h3 class="mb-0">Adaptor: Objective-Centric Adaptation Library for Language Models</h3>
            <div class="subheading mb-3"><span class="text-primary">60th Annual Meeting of the ACL, May 2022</span>

                <br></br>
                <p>Blitz 2min introduction video of Adaptor library presented on ACL 2022 in Dublin.</p>
                <iframe width="560" height="315" src="https://www.youtube.com/embed/soP6gIVr46E"
                        title="YouTube video player" frameborder="0"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                        allowfullscreen></iframe>
                <br></br>
            </div>
        </div>

    </section>
    <hr class="m-0"/>

    <!-- Other -->
    <section class="resume-section" id="other">
        <div class="resume-section-content">
            <h2 class="mb-5">Other Activities</h2>

            <div class="d-flex flex-column flex-md-row justify-content-between">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Lecturer</h3>
                    <div class="subheading mb-3">Course: Introduction to Information Retrieval</div>
                    <p>Over the three years, I have given practicals for 100+ master's students on the essentials of
                        text representations,
                        indexing methods or machine learning approaches to a full-text search.</p>
                    <div>I have also initiated the redesign of the course to a semestral competition based on an
                        easy-to-use
                        <a href="https://github.com/MIR-MU/pv211-utils">evaluation framework</a>.
                        Given the results of semestral student surveys, the competition model enhanced the students'
                        engagement and interest in the NLP topics. The competition results also allow us to reach out to
                        the best students with an offer for further research cooperation.
                    </div>

                    <br></br>
                </div>

                <div class="flex-shrink-0"><span class="text-primary">Springs 2019 - now</span></div>
            </div>
            <div class="d-flex flex-column flex-md-row justify-content-between">
                <div class="flex-grow-1">
                    <h3 class="mb-0" id="supervisor">Supervisor</h3>
                    <div class="subheading mb-3">Transformers Club & others</div>
                    <p>I have supervised numerous bachelor's and master's theses on topics closely related
                        to the robustness of language models. For the more ambitious students, I organize the weekly
                        meetings of <strong>Transformers Club</strong>, a shared platform for pursuing creative
                        ideas, peer reviewing and organizing teams to address more complex problems.</p>
                    <p>The theses that emerged from the Club and/or I have supervised:</p>
                    <ul>
                        <li>Marek Kadlƒç√≠k (current): on <a href="https://arxiv.org/abs/2305.15017">enhancing language models by interaction with symbolic systems</a>.</li>
                        <li>≈†√°rka ≈†ƒçavnick√° (current): on <a href="https://nlp.fi.muni.cz/raslan/2022/paper17.pdf">document understanding through visual question answering</a>.</li>
                        <li>D√°vid Melu≈° (current): on <a href="https://huggingface.co/fimu-docproc-research/standard_0.4.0_EasyOcrEngine">enhancing quality of optical character recognition</a> with specialized language models.</li>
                        <li>Luk√°≈° Mikula: <a href="https://is.muni.cz/th/adh58/?lang=en">Think Twice Before You Answer:
                            Mitigating Biases of Question Answering Models</a> (2022);
                            ü•á 1st place in <a href="https://dadcworkshop.github.io/">NAACL DADC</a> shared task Track 2
                            &
                            üèÜ a <a href="https://www.fi.muni.cz/about/awards/awards.html.en">Dean's award</a>.
                        </li>
                        <li>Marek Petroviƒç: <a href="https://is.muni.cz/th/htvm4/?lang=en">One Bit at a Time: Impact of
                            Quantisation on Neural Machine Translation</a> (2022);
                            üèÜ a <a href="https://www.fi.muni.cz/about/awards/awards.html.en">Dean's award</a> &
                            ü•â 3rd place in <a href="http://math.sk/svoc2022/#vysledky">SVOC</a> competition.
                        </li>
                        <li>Matej Me≈°ko: <a href="https://is.muni.cz/th/utq47/?lang=en">Evaluation of a Supervised
                            Approach to Information Retrieval</a> (2022)
                        </li>
                        <li>Martin Geletka: <a href="https://is.muni.cz/th/if8vh/?lang=en">Speeding up inference time of
                            neural machine translation</a> (2021);
                            üéñ Nomination for a Dean's award.
                        </li>
                        <li>Petr Miƒçka: <a href="https://is.muni.cz/th/z52vo/?lang=en">Utilisation of language
                            representations for Information Retrieval</a> (2021)
                        </li>

                    </ul>
                    <br></br>
                </div>
                <div class="flex-shrink-0"><span class="text-primary">Autumn 2020 - now</span></div>
            </div>

            <div class="d-flex flex-column flex-md-row justify-content-between">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Reviewer</h3>
                    <div class="subheading mb-3">*ACL Conferences, ACL Rolling Review & other</div>
                    <p>I give back to the community by volunteering for reviewing, proofreading and providing feedback
                        to other researchers whenever it can help. Feel free to get in touch if you're interested!
                        <br></br>
                        <br></br>
                </div>

            </div>

        </div>
    </section>

    <!-- Other -->
    <section class="resume-section" id="afk">
        <div class="resume-section-content">
            <h2 class="mb-5">Away From Keyboard</h2>
            <br></br>
            <br>Reading the most recent pre-prints is fun, but there's a lot of other great stuff to see and
            do.</br>
            <br>Whenever possible, I love spending time moving with unlimited open space ‚òÅ ‚¨Ü above my head, best done on
            a
            bike üö≤ or skis ‚õ∑, with far mountain views ‚õ∞‚òÄÔ∏è and a taste of a filtered brew ‚òï .</br>
        </div>


    </section>
</div>
</section>
<hr class="m-0"/>
</div>
<!-- Bootstrap core JS-->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
<!-- Core theme JS-->
<script src="js/scripts.js"></script>
</body>
</html>

