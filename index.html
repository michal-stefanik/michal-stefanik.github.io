<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/html">
<head>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/>
    <meta name="description" content=""/>
    <meta name="author" content=""/>
    <title>Michal Stefanik</title>
    <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico"/>
    <!-- Font Awesome icons (free version)-->
    <script src="https://use.fontawesome.com/releases/v6.1.0/js/all.js" crossorigin="anonymous"></script>
    <!-- Google fonts-->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet"
          type="text/css"/>
    <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css"/>
    <!-- Core theme CSS (includes Bootstrap)-->
    <link href="css/styles.css" rel="stylesheet"/>
</head>
<body id="page-top">
<!-- Navigation-->
<nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
    <a class="navbar-brand js-scroll-trigger" href="#page-top">
        <span class="d-block d-lg-none">Michal ≈†tef√°nik</span>
        <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2"
                                             src="assets/img/profile.jpg" alt="..."/></span>
    </a>
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive"
            aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span
            class="navbar-toggler-icon"></span></button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav">
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#publications">Research</a></li>
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#experience">Experience</a></li>
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#talks">Talks</a></li>
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#other">Other</a></li>
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#afk">Other other</a></li>
            <!--                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#awards">Awards</a></li>-->
        </ul>
    </div>
</nav>
<!-- Page Content-->
<div class="container-fluid p-0">
    <!-- About-->
    <section class="resume-section" id="about">
        <div class="resume-section-content">
            <h1 class="mb-0">
                <!--                        <span class="text-primary">Michal ≈†tef√°nik</span>-->
                Michal ≈†tef√°nik
            </h1>
            <div class="subheading mb-5">
                Researcher in Artificial Intelligence & Natural Language Processing
                <p><a href="mailto:stefanik.m@mail.muni.cz"> stefanik.m@mail.muni.cz</a></p>
            </div>
            <div class="social-icons">
                <a class="social-icon" href="https://scholar.google.com/citations?user=9p-110IAAAAJ&hl=en&oi=ao"><img
                        width="48%" height="48%" object-fit="contain" src="assets/img/gscholar-icon.png"/></a>
                <a class="social-icon" href="https://www.linkedin.com/in/stefanikm"><i
                        class="fab fa-linkedin-in"></i></a>
                <a class="social-icon" href="https://github.com/stefanik12"><i class="fab fa-github"></i></a>
                <a class="social-icon" href="https://www.facebook.com/stefanik.mich"><i
                        class="fab fa-facebook-f"></i></a>
                <p></p>
                <p></p>
            </div>
            <p class="lead mb-5"></p>
            <p class="lead mb-6">Welcome to my webpage! I am a last-year PhD Researcher at the <a
                    href="https://www.fi.muni.cz/about/">Faculty of Informatics</a> of
                <a href=https://www.muni.cz/en>Masaryk University</a>
                and an NLP Team Lead at <a href="https://www.gaussalgo.com/en/">Gauss Algorithmic</a>.
            </p>
            <p class="lead mb-6">
                My research focus is on the <strong>robustness of language
                models</strong>.
                This includes a reliable <strong>evaluation</strong> of models' generalization
                but also methods to <strong>improve</strong> models' reliability
                by curating better data or improving existing training methods.
                My research, presented at highly selective *ACL conferences, delivers better models for <strong>low-resource</strong> applications,
                instructional models with more <strong>human-like reasoning</strong>,
                or question-answering systems robust to <strong>prediction shortcuts</strong>.
            </p>
            <p class="lead mb-6">
                I am a proud founder and leader of <a href="#supervisor">TransformersClub</a>‚Ñ¢, a platform for supporting students
                in pursuing research ideas, with alumni awarded with international prizes and papers in top-tier NLP/AI conferences.
            </p>
            <p class="lead mb-6">
                Over the last six years, I've also contributed to and led the delivery of many <strong>industrial
                applications</strong> of cutting-edge language technologies,
                including multilingual <a href="https://www.gaussalgo.com/en/case-studies/lectura-and-seo-text-optimalization">language
                generation</a> or
                <a href="https://www.gaussalgo.com/en/cyber-security/revealing-sensitive-documents-with-ner-practical-case-study">entity
                    recognition</a>,
                all the way from the research ideas to the scalable deployments, now serving
                the fascinating NLP technologies to thousands of users.
            </p>
            <p class="lead mb-6">
                I expect to obtain a PhD at the end of 2024 and am looking for new research challenges in the
                following time period!
            </p>
        </div>
    </section>
    <hr class="m-0"/>
    <!-- Experience-->
    <section class="resume-section" id="publications">
        <div class="resume-section-content">
            <h2 class="mb-5">Research</h2>
            <p>Below you can find references to my most recent publications.
                An overview of my research can now also be found in my <a href="https://is.muni.cz/th/m805b/PhD_thesis_Michal_Stefanik.pdf">dissertation</a> üìÑ
            </p>
            </br>
            <h3 class="mb-2"><span class="text-primary">2024</span></h3>

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Self-Training Language Models in Arithmetic Reasoning</h3>
                    <div class="subheading mb-3">Marek Kadlƒç√≠k, <span class="text-primary">Michal ≈†tef√°nik</span>
                        (equal)
                    </div>
                    <div class="lead mb-3">In <a href="https://aclanthology.org/2024.findings-emnlp.721">Proceedings of
                        EMNLP 2024: Findings track</a> &
                        <a href="https://openreview.net/forum?id=zBh79GuLNO">ICLR 2024 LLMAgents</a>.
                    </div>
                    <p>Language models achieve impressive results in tasks involving complex multistep reasoning,
                        but scaling these capabilities further traditionally requires expensive collection of more
                        annotated data. In this work, we explore the potential of improving the capabilities of
                        language models using automated feedback based on the validity of their predictions in
                        arithmetic reasoning (self-training).
                    </p>
                    <p>
                        We find that models can substantially improve in both single-round (offline) and online
                        self-training. In the offline setting, supervised methods are able to deliver gains
                        comparable to preference optimization, but in online self-training, preference optimization
                        shows to largely outperform supervised training thanks to superior stability and robustness
                        on unseen types of problems.
                    </p>
                </div>
            </div>

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Concept-aware Data Construction Improves In-context Learning of Language
                        Models</h3>
                    <div class="subheading mb-3"><span class="text-primary">Michal ≈†tef√°nik</span>, Marek Kadlƒç√≠k, Petr
                        Sojka
                    </div>
                    <div class="lead mb-3">In <a href="https://aclanthology.org/2024.findings-acl.733">Proceedings of
                        ACL 2024: Findings track</a> &
                        <a href="https://openreview.net/forum?id=zBh79GuLNO">ICLR 2024 Understanding of Foundation Models</a>
                        ¬∑ üìÑ
                        <a href="https://michal-stefanik.medium.com/what-makes-a-language-model-to-understand-your-instruction-d0ce384930a2">Blog
                            post</a>.
                    </div>
                    <p>Previous work curating in-context learning models assumes that ICL emerges from vast
                        over-parametrization or
                        the scale of multitask training. However, theoretical work
                        attributes ICL emergence to specific properties of training data and creates functional
                        in-context learners in small-scale, synthetic settings.
                    </p>
                    <p>
                        Inspired by previous theories, we propose a method to construct training data requiring analogical reasoning.
                        We show the surprising efficiency of training on such data, with resulting models significantly outperforming
                        classic instruction tuning in 41 and 45 out of 60 tasks, while performing on par with
                        models trained on over 1600 tasks while using only two (2) QA tasks.
                        Our analyses attribute these improvements to enhanced robustness of our models towards shortcuts
                        of previous ICL models and improved ability to benefit from informative demonstrations.
                    </p>
                </div>
            </div>

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of
                        Question Answering Models</h3>
                    <div class="subheading mb-3">Luk√°≈° Mikula*, <span class="text-primary">Michal ≈†tef√°nik</span>*,
                        Marek Petroviƒç, Petr Sojka (*equal)
                    </div>
                    <div class="lead mb-3">In <a href="https://arxiv.org/abs/2305.06841">Proceedings of EACL 2024: Main
                        track</a> ¬∑ ü•á Best submission in NAACL DADC: <a
                            href="https://dadcworkshop.github.io/shared-task/">Better training data</a> challenge
                    </div>
                    <!--                    <div class="lead mb-3"></div>-->
                    <p>In this paper, we challenge the commonly-used evaluation of robustness of language models through
                        the lens of their out-of-distribution performance. We propose a metric to measure the reliance
                        of
                        model's performance on a specific feature and measure the reliance on a set of features
                        identified as
                        prediction shortcuts.
                    </p>
                    <p>
                        We find that the same prediction shortcuts are exposed in all assessed QA datasets,
                        whereas some shortcuts identified in SQuAD are even more impactful in other datasets.
                        This means that counterintuitively, a model relying on shortcut more might reach better OOD
                        performance.
                        Finally, we survey three existing debiasing methods and find that indeed, the OOD gains of these
                        methods
                        come in hand with increased, rather than decreased reliance on prediction shortcuts.
                    </p>
                    <p><a href="#eacl24_talk">Conference talk ‚¨áÔ∏è</a>Ô∏è & <a href="#dadc_results">Announcement of results
                        in NAACL DADC ‚¨áÔ∏è</a>Ô∏è</p>
                </div>
            </div>

            <h3 class="mb-2"><span class="text-primary">2023</span></h3>

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought
                        through Interaction with Symbolic Systems</h3>
                    <div class="subheading mb-3">Marek Kadlƒç√≠k*, <span class="text-primary">Michal ≈†tef√°nik</span>*,
                        Ond≈ôej Sotol√°≈ô, Vlastimil Martinek (*equal)
                    </div>
                    <div class="lead mb-3">In <a href="https://aclanthology.org/2023.emnlp-main.742">
                        Proceedings of EMNLP 2023: Main track</a>
                    </div>
                    <p>Language models are notoriously
                        inclined to make factual errors in tasks requiring arithmetic reasoning.
                        To enable language models to circumvent this deficiency and offload a critical computation to a
                        symbolic system, we create a collection of Calc-X datasets that demonstrate the appropriate use
                        of a calculator in reasoning chains.
                    </p>
                    <p>
                        We survey and unify several existing chain-of-thought datasets into a proposed novel format,
                        resulting in a standard collection of over 300,000 samples requiring arithmetic reasoning.
                        Finally, we use the new collection to train open-source calculator-assisted language models and
                        show that models trained on Calc-X almost double the accuracy of generating correct results
                        compared to baselines, and can outperform larger models of previous work, such as Toolformer, or
                        Llama 2.
                        We make all Calc-X <a
                            href="https://huggingface.co/collections/MU-NLPC/calc-x-652fee9a6b838fd820055483">datasets</a>
                        and <a href="https://huggingface.co/collections/MU-NLPC/calcformers-65367392badc497807b3caf5">models</a>
                        publicly available</a>.
                    </p>
                    <p>Link to <a href="https://aclanthology.org/2023.emnlp-main.742.mp4">Marek's EMNLP talk üé•</a></p>
                </div>
            </div>


            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Can In-context Learners Learn a Reasoning Concept from Demonstrations?</h3>
                    <div class="subheading mb-3"><span class="text-primary">Michal ≈†tef√°nik</span>, Marek Kadlƒç√≠k</div>
                    <div class="lead mb-3">In <a href="https://arxiv.org/abs/2212.01692">Proceedings of ACL 2023 Natural
                        Language Reasoning (NLRSE) workshop</a> ¬∑ ü•á Best paper award
                    </div>
                    <p>Recent work analysing the functioning of in-context learners show that instead of learning new
                        associations from the input,
                        models largely rely on their pre-trained knowledge, such as the sentiment of the labels.
                    </p>
                    <p>
                        We argue that evaluations using a randomly-chosen demonstrations can not disentangle models'
                        reliance on pre-trained knowledge
                        from the ability to learn new functional relations, as most of such demonstrations can provide
                        only limited information.
                        Hence, we propose to evaluate in-context learners with demonstrations sharing a specific,
                        informative reasoning concept with predicted sample.
                        We find that most of the recent in-context learners can not consistently benefit from the
                        demonstrated concepts, irrespective of the model size.
                        However, some models are more sensitive to concepts than others, such as T0 models, which can
                        benefit from concepts in 7 of 8 evaluation scenarios.
                    </p>
                </div>
            </div>

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Soft Alignment Objectives for Robust Adaptation of Language Generation</h3>
                    <div class="subheading mb-3"><span class="text-primary">Michal ≈†tef√°nik</span>, Marek Kadlƒç√≠k, Petr
                        Sojka
                    </div>
                    <div class="lead mb-3">In <a href="https://aclanthology.org/2023.acl-long.492">Proceedings of the
                        61th Annual
                        Meeting of the ACL (ACL 2023): Main track</a>
                        ¬∑ üìÑ
                        <a href="https://medium.com/@michal-stefanik/training-for-single-correct-prediction-makes-your-model-more-fragile-7e74a09fb4d9">Blog
                            post</a></div>
                    <p>Fine-tuning of pre-trained generative language models weakens their ability to
                        generalize, making the open-ended deployment of these models prone to errors like hallucinations
                        or degradations of output text quality.
                    </p>
                    <p>
                        In this work, we show that adapting the models with modeling the ambiguity of prediction
                        can largely avoid over 95% of the loss caused by traditional fine-tuning.
                    </p>
                    <p><a href="#soft_align_talk">Conference talk ‚¨áÔ∏è</a>Ô∏è</p>
                </div>
            </div>

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Resources and Few-shot Learners for In-context Learning in Slavic Languages</h3>
                    <div class="subheading mb-3"><span class="text-primary">Michal ≈†tef√°nik</span>, Marek Kadlƒç√≠k, Piotr
                        Gramacki, Petr Sojka
                    </div>
                    <div class="lead mb-3">In <a href="https://aclanthology.org/2023.bsnlp-1.12/">Proceedings of EACL
                        SlavicNLP 2023 workshop</a> ¬∑ ü•á Best paper award
                    </div>
                    <p>In this work, we collect the infrastructure necessary for training and evaluation of ICL
                        in a selection of Slavic languages: Czech, Polish, and Russian. We link a diverse set of
                        datasets and cast these into a unified instructional format through a set of transformations
                        and newly-crafted templates written purely in target languages.Using the newly-curated dataset,
                        we evaluate a set of the most recent in-context learners and compare their results to the
                        supervised baselines. Finally, we train, evaluate and publish a set of in-context learning
                        models.
                    </p>
                    <p>
                        We find that the massive multitask training can be outperformed by single-task training in the
                        target language, uncovering the potential for specializing in-context learners to the
                        language(s) of their application.

                    </p>
                </div>
            </div>


            <h3 class="mb-2"><span class="text-primary">2022</span></h3>

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Applications of deep language models for reflective writings</h3>
                    <div class="subheading mb-3">Jan Nehyba, <span class="text-primary">Michal ≈†tef√°nik</span> (equal)
                    </div>
                    <div class="lead mb-3">In <a href="https://rdcu.be/cUWGY">Education and Information Technologies</a>
                        (IF 5.5)
                    </div>
                    <p>Social sciences exhibit many cognitively complex and highly qualified
                        problems, whose resolution relies on often subjective expert judgements.
                        One of such problems that we study in this work is a
                        reflection analysis in the writings of student teachers.
                    <p>
                    </p>
                    We perform a variety of experiments on how to efficiently address data collection for applications
                    exhibiting a great level of annotators' subjectivity.
                    Additionally, we demonstrate a great potential in a cross-lingual transfer of multilingual language
                    models
                    in the analysis of reflective writing.
                    We make our resulting datasets and models <a
                        href="https://github.com/EduMUNI/reflection-classification">freely available</a>.</p>
                </div>
            </div>

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Methods for Estimating and Improving Robustness of Language Models</h3>
                    <div class="subheading mb-3"><span class="text-primary">Michal ≈†tef√°nik</span></div>
                    <div class="lead mb-3">In <a href="https://aclanthology.org/2022.naacl-srw.6/">Proceedings of NAACL
                        2022: Student Research Workshop</a></div>
                    <p>Despite their outstanding performance, large language models (LLMs) suffer notorious flaws
                        related to their preference for simple, surface-level textual relations over full semantic
                        complexity of the problem. This proposal investigates a common denominator of this problem in
                        their weak ability to generalise outside the training domain.
                    </p>
                    <p>
                        We survey diverse research
                        directions providing estimations of model generalisation ability and find that incorporating
                        some of these measures in the training objectives leads to enhanced distributional robustness of
                        neural models. Based on these findings, we present future research directions towards enhancing
                        the robustness of LLMs.</p>
                    <p><a href="#srw_talk">Conference talk ‚¨áÔ∏è</a>Ô∏è</p>
                </div>
            </div>
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Adaptor: Objective-Centric Adaptation Library for Language Models</h3>
                    <div class="subheading mb-3"><span class="text-primary">Michal ≈†tef√°nik</span>, V√≠t Novotn√Ω, Nikola
                        Groverov√°, Petr Sojka
                    </div>
                    <div class="lead mb-3">In <a href="https://aclanthology.org/2022.acl-demo.26/">Proceedings of the
                        60th Annual Meeting of the ACL: Demonstrations</a></div>
                    <p>This paper introduces Adaptor library, transposing the traditional model-centric approach
                        composed of pre-training + fine-tuning steps to objective-centric approach, composing the
                        training from applications of selected objectives. We survey research directions that can
                        benefit from enhanced objective-centric experimentation in multitask training, custom objectives
                        development, dynamic training curricula, or domain adaptation and demonstrate the practical
                        applicability of Adaptor in selected unsupervised domain adaptation scenarios.</p>
                    <p><a href="#adaptor_talk">Introduction talk ‚¨áÔ∏è</a>Ô∏è</p>
                </div>
            </div>
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">When FastText Pays Attention: Efficient Estimation of Word Representations using
                        Constrained Positional Weighting</h3>
                    <div class="subheading mb-3">V√≠t Novotn√Ω, <span class="text-primary">Michal ≈†tef√°nik</span>, Eniafe
                        Festus Ayetiran, Petr Sojka, Radim ≈òeh≈Ø≈ôek
                    </div>
                    <div class="lead mb-3">In <a href="https://lib.jucs.org/article/69619/">JUCS: The Journal of
                        Universal Computer Science</a> (IF 1.3)
                    </div>
                    <p>We propose a constrained positional model, which adapts the sparse attention mechanism from
                        neural machine translation to improve the speed of the positional model. Our constrained model
                        outperforms the positional model on language modelling and trains twice as fast.</p>
                </div>
            </div>

            <h3 class="mb-2"><span class="text-primary">2021</span></h3>

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">RegEMT: Regressive Ensemble for Machine Translation Quality Evaluation</h3>
                    <div class="subheading mb-3"><span class="text-primary">Michal ≈†tef√°nik</span>, V√≠t Novotn√Ω, Petr
                        Sojka
                    </div>
                    <div class="lead mb-3">In <a href="https://aclanthology.org/2021.wmt-1.112/">Proceedings of the
                        Sixth Conference on Machine Translation (WMT)</a>
                        ¬∑ ü•á <a href="https://aclanthology.org/2021.wmt-1.73v3.pdf#page=10">Best-performing
                            reference-free metric</a></div>
                    <p>This work introduces a simple regressive ensemble for evaluating machine translation quality
                        based on a set of novel and established metrics. We evaluate the ensemble using a correlation to
                        expert-based MQM scores of the WMT 2021 Metrics workshop.
                    </p>
                    <p>
                        In both monolingual and zero-shot
                        cross-lingual settings, we show a significant performance improvement over single metrics. In
                        the cross-lingual settings, we also demonstrate that an ensemble approach is well-applicable to
                        unseen languages. Furthermore, we identify a strong reference-free baseline that consistently
                        outperforms the commonly-used BLEU and METEOR measures and significantly improves our ensemble‚Äôs
                        performance.</p>
                </div>
            </div>

            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Ensembling Ten Math Information Retrieval Systems: MIRMU and MSM @ ARQMath
                        2021</h3>
                    <div class="subheading mb-3">V√≠t Novotn√Ω,
                        <span class="text-primary">Michal ≈†tef√°nik</span>, D√°vid Lupt√°k, Martin Geletka, Petr Sojka
                    </div>
                    <div class="lead mb-3">In <a href="https://ceur-ws.org/Vol-2936/paper-06.pdf">
                        CLEF Lab on Answer Retrieval for Questions on Math (ARQMath@CLEF 2021)</a>
                        ¬∑ ü•á <a href="https://ceur-ws.org/Vol-3180/paper-01.pdf#page=24">Best automatic CQA system</a>
                    </div>

                    <p>ARQMath Community Question Answering (CQA) competition challenges open-domain QA systems to find
                        answers to a set of
                        yet non-answered questions on Math Stack Exchange, potentially
                        providing users with responses to all new but answerable questions.
                    </p>
                    <p>
                        In our submission, we create an ensemble of ten ‚Äúweak‚Äù individual systems
                        that we let vote to provide answers to unseen questions. Our submission, a best-performing among
                        all the automated systems, shows that such ensemble approach can be more robust
                        on unseen questions than single-model neural systems.

                    </p>
                </div>
            </div>


        </div>
    </section>
    <hr class="m-0"/>
    <!-- Experience-->
    <section class="resume-section" id="experience">
        <div class="resume-section-content">
            <h2 class="mb-5">Experience</h2>
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <h3 class="mb-0">NLP Scientist - Team Lead</h3>
                    <div class="subheading mb-3">Gauss Algorithmic</div>
                    <div>Delivering the most recent NLP technologies from whiteboards to real-world users.
                    </div>
                    <br>‚Ä¢ Research and productisation of language technologies for diverse specialized applications, involving language generation, classification, entity recognition, search or recommendation.
                    </br>
                    <br>‚Ä¢ On-premise deployments of generative language models: compute optimisation, scalability, high availability assurance.
                    </br>
                    <br>‚Ä¢ Coordinating a diverse team of tenure NLP scientists, software developers, operations engineers, business developers, and copywriters. </br>
                    <br>
                    ‚Ä¢ Communication with clients of diverse cultural backgrounds, <a href="#talks">public talks</a>, and <a href="https://github.com/gaussalgo/L2L_MLPrague23">hands-on workshops</a> at tech conferences and companies.
                    <br/>
                    <br></br>
                    <div class="subheading mb-md-1"><span class="text-primary">Case Studies</span></div>
                    <br>‚Ä¢ We developed a paraphraser that can transcribe technical documentation to a more familiar language,
                    enabling easier comprehension and searchability by visitors.
                    Our solution enabled our customer to attract 12% more visitors to their websites.</br>

                    <br>‚Ä¢ We delivered machine translation models specialized for chat conversations across languages of
                    Southeast Asia. Our on-premise deployment enables our customer to save 10k+ USD per month while
                    delivering translation quality comparable to Google Translate.
                    </br>
                    <br>See our <a href="https://www.gaussalgo.com/insights">blogs</a>, <a
                        href="https://www.gaussalgo.com/case-studies/seo-text-proofreading-and-optimization">NLP case
                    studies</a>, <a href="https://huggingface.co/gaussalgo">HuggingFace models</a> or <a
                        href="https://github.com/gaussalgo/adaptor">open-source projects</a>.</br>
                </div>
                <div class="flex-shrink-0"><span class="text-primary">March 2021 - now</span></div>
            </div>


            <div class="d-flex flex-column flex-md-row justify-content-between">

                <div class="flex-grow-1">
                    <h3 class="mb-0">Technical lead</h3>
                    <div class="subheading mb-3">Grant Project: Intelligent Back Office</div>
                    <p>Coordination of the research team of 5-6 researchers within a larger, multi-organization
                        <a href="https://www.muni.cz/en/research/projects/64989">grant project</a> focused on improving
                        document processing quality for specialised domain(s).</p>
                    <div>Within this project, we (1) enhanced the quality of domain-specific OCR text extraction
                        utilising semi-supervised language modelling and (2) improved the methods to utilise
                        positional information of document segments for a more accurate identification of the named
                        entities.
                    </div>
                    <br></br>
                </div>
                <div class="flex-shrink-0"><span class="text-primary">September 2021 - May 2023</span></div>

            </div>


            <div class="d-flex flex-column flex-md-row justify-content-between">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Deep Learning Engineer</h3>
                    <div class="subheading mb-3">Gauss Algorithmic</div>
                    <div>Creating the prototypes of NLP applications for specific use-cases,
                        in multilingual classification, named entity recognition, or language generation.
                    </div>
                    <br>‚Ä¢ Development of custom architectures and data pipelines.</br>
                    <br>‚Ä¢ Streamlining the experimentation, setting up practices in model and data versioning, reproducibility, integration.</br>
                    <br>‚Ä¢ Implementation of the essential infrastructure around Transformer-based models (classification, generation)
                    in the early Transformers era.</br>
                    <br>Implementations in Tensorflow and later PyTorch, deployments using CI, Docker & Kubernetes in AWS and
                    Google Cloud.</br>
                    <br></br>
                </div>
                <div class="flex-shrink-0"><span class="text-primary">June 2018 - March 2021</span></div>
            </div>

            <div class="d-flex flex-column flex-md-row justify-content-between">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Junior Software Developer</h3>
                    <div class="subheading mb-3">Red Hat Software</div>
                    <div>Enhancing the company search engines with semantic text representations and classification,
                        within the <a href="https://github.com/searchisko/project-classifier-poc">Searchisko</a> & other
                        open-source projects.
                    </div>
                    <br>Application of pivotal semantic representation technologies such as Word2Vec, and FastText. Implementations in Python &
                    Java, deployments in OpenShift.
                    </br>
                    <br></br>
                </div>
                <div class="flex-shrink-0"><span class="text-primary">March 2015 - June 2018</span></div>
            </div>
            <div class="d-flex flex-column flex-md-row justify-content-between">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Stream Processing Developer</h3>
                    <div class="subheading mb-3">CSIRT: Cyber Security Incident Response Team of Masaryk Univerity</div>
                    <div>Research & development of intrusion detection methods in scalable, streaming paradigms.</div>
                    <br>Implementations of unsupervised and semi-supervised Machine Learning methods in Apache Spark &
                    Python.
                    Most of our research is open-sourced in <a href="https://github.com/CSIRT-MU/Stream4Flow">Stream4Flow</a>
                    project.
                    </br>

                </div>
                <div class="flex-shrink-0"><span class="text-primary">March 2016 - September 2017</span></div>
            </div>
        </div>
    </section>
    <hr class="m-0"/>
    <!-- Talks -->
    <section class="resume-section">
        <div class="resume-section-content">
            <h2 class="mb-5" id="talks">Talks</h2>

            <h3 id="helsinki25_talk" class="mb-0">Reliable Language Models and Where to Find Them</h3>
            <div class="subheading mb-3"><span class="text-primary">Language Technology seminar, University of Helsinki, March 2025</span>
                <br></br>
                <p>Lightening talk covering most recent work towards improving LMs reliability<br>
                    <a href="https://drive.google.com/file/d/12zU-0PqbFET0_gwMfo2o8Xm6CzLrZwJQ/view?usp=sharing">Presentation
                        slides</a> with links</p>
                <img alt="MLMU Cover Photo"
                     src="assets/img/helsinki_25_seminar.png" loading="eager"
                     class="rounded-full object-cover" style="width: 560px; height: 315px;">
                <br></br>
            </div>
            
            <h3 id="colloquium_talk" class="mb-0">Fantastically Robust Language Models and Where to Find Them</h3>
            <div class="subheading mb-3"><span class="text-primary">Invited talks: Informatics Colloquium at Faculty of Informatics, Masaryk University <br>Allegro NLP Seminar, Online</span>
                <br></br>
                <p>Presentation slides:
                    <a href="https://docs.google.com/presentation/d/1AZvzolNFlNzWAlqS1dnsyoninAjcgHfm7tF5BBM68fk/edit?usp=sharing">20min version</a>
                    &
                    <a href="https://docs.google.com/presentation/d/18jtr0aIeshW8fXBYsGBMejucD60XIK6ygv47QdzKrbw/edit?usp=sharing">1h version</a>.
                </p>
                <img alt="DataMesh Presentation Cover Photo"
                     src="assets/img/FI_colloquium.png" loading="eager"
                     class="rounded-full object-cover" style="width: 560px; height: 315px;">
                <br></br>

            </div>

            <h3 id="datamesh_talk" class="mb-0">Language Models in Autonomous Systems</h3>
            <div class="subheading mb-3"><span class="text-primary">Community talk: DataMesh Brno, September 2024 (Offline)</span>
                <br></br>
                <p>Informal talk sharing our experience with deployments of custom models, including applications of
                    autonomous decision agents.<br>
                    <a href="https://drive.google.com/file/d/1sg-9cAXtXP0G3U6TDSbS17z6f9Holy1r/view?usp=sharing">Presentation
                        slides (in Slovak)
                    </a>
                </p>
                <img alt="DataMesh Presentation Cover Photo"
                     src="assets/img/datamesh_cover.png" loading="eager"
                     class="rounded-full object-cover" style="width: 560px; height: 315px;">
                <br></br>

            </div>

            <h3 id="haiss_talk" class="mb-0">Training for Single Correct Prediction Makes Your Model More Fragile</h3>
            <div class="subheading mb-3"><span class="text-primary">HumanAligned.ai Summer school in Prague, July 2024 (Offline)</span>
                <br></br>
                <p>A short teaser talk for our <a href="#soft_align_talk">Soft Alignment Objectives</a> presented at ACL
                    2023.<br>
                    <a href="https://drive.google.com/file/d/1gdzQGS6TSr-vLC9g6hnIgYq8ocp7jegu/view?usp=sharing">Presentation
                        slides
                    </a>,
                    <a href="https://michal-stefanik.medium.com/training-for-single-correct-prediction-makes-your-model-more-fragile-7e74a09fb4d9">Medium
                        blog post
                    </a>
                </p>
                <img alt="HAISS Presentation Cover Photo"
                     src="assets/img/haiss_talk_cover.png" loading="eager"
                     class="rounded-full object-cover" style="width: 560px; height: 315px;">
            </div>


            <h3 id="eacl24_talk" class="mb-0">Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts
                of QA Models</h3>
            <div class="subheading mb-3"><span
                    class="text-primary">EACL 2024 Main conference talk in Malta, March 2024</span>
                <br></br>
                <p>A 12 min overview of our work exploring the limits of evaluating robustness of language models in
                    Question answering.</p>
                <iframe width="560" height="315" src="https://www.youtube.com/embed/6w-sh8OMCBo"
                        title="YouTube video player" frameborder="0"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                        allowfullscreen></iframe>
                <br></br>
            </div>

            <h3 class="mb-0">Robustness of Language Models and Perspectives of Modularization</h3>
            <div class="subheading mb-3"><span class="text-primary">Language Technology seminar, University of Helsinki, March 2024</span>
                <br></br>
                <p>A presentation of my talk covering recent methods for improving robustness of language models and the
                    role of modularization.<br>
                    <a href="https://drive.google.com/file/d/1ilY7YwmXCw2F4S824cGStI4NR7WwKTQT/view?usp=drive_link">Presentation
                        slides</a> with links to the literature.</p>
                <img alt="MLMU Cover Photo"
                     src="assets/img/Helsinki_NLP_talk_title.png" loading="eager"
                     class="rounded-full object-cover" style="width: 560px; height: 315px;">
                <br></br>
            </div>


            <h3 class="mb-0" id="soft_align_talk">Soft Alignment Objectives for Robust Adaptation of Language
                Generation</h3>
            <div class="subheading mb-3">
                <span class="text-primary">ACL 2023 conference in Toronto, Canada, July 2023</span>
                <br></br>
                <p>A 6 min exposition of our ACL paper proposing more robust training objectives for language
                    generation.</p>
                <p>See also a <a
                        href="https://medium.com/@michal-stefanik/training-for-single-correct-prediction-makes-your-model-more-fragile-7e74a09fb4d9">blogpost</a>
                    on the topic</p>
                <iframe width="560" height="315" src="https://www.youtube.com/embed/YWHGuichMCA"
                        title="YouTube video player" frameborder="0"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                        allowfullscreen></iframe>
                <br></br>
            </div>

            <h3 class="mb-0"  id="l2l_talk">Learning to Learn: Hands-on Tutorial in Using and Improving Few-shot Language Models</h3>
            <div class="subheading mb-3"><span class="text-primary">Workshop on Machine Learning Prague conference, in Prague, Czechia, June 2023</span>
                <br></br>
                <p>Organisation of the workshop covering most recent topics with hands-on tutorials in in-context
                    learning.<br>
                    <a href="https://github.com/gaussalgo/L2L_MLPrague23">Workshop github</a> &
                    <a href="https://github.com/gaussalgo/L2L_MLPrague23/blob/main/presentation.pdf">Presentation
                        slides</a> with links to the literature.</p>
                <img alt="MLMU Cover Photo"
                     src="assets/img/mlprague.jpeg" loading="eager"
                     class="rounded-full object-cover" style="width: 560px; height: 315px;">
                <br></br>
            </div>


            <h3 class="mb-0">Resources and Few-shot Learners for In-context Learning in Slavic Languages</h3>
            <div class="subheading mb-3"><span class="text-primary">EACL 2023: Slavic NLP workshop in Dubrovnik, Croatia, May 2023 ¬∑ ü•á Best paper award</span>
                <br></br>
                <p>A 10 min overview of our work in building and evaluating in-context learning in Slavic languages.</p>
                <iframe width="560" height="315" src="https://www.youtube.com/embed/Sun2FhQTQZI"
                        title="YouTube video player" frameborder="0"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                        allowfullscreen></iframe>
                <br></br>
            </div>

            <h3 class="mb-0">Learning to Learn: Training Language Models to Understand Tasks from Few Examples</h3>
            <div class="subheading mb-3"><span class="text-primary">Community talk: Machine Learning MeetUp Brno, October 2022 (Offline)</span>
                <br></br>
                <p>Informal presentation of our methodology and trained models for Few-shot in Czech and other
                    Non-English languages.<br>
                    <a href="https://michal-stefanik.medium.com/few-shot-learning-goes-multilingual-9837b2962901">Medium
                        Blog</a> &
                    <a href="https://www.facebook.com/events/6396221753725366/?active_tab=about">Event</a> &
                    <a href="https://drive.google.com/file/d/1xO7V0AmE57eH4FAoH517XZPQ0_DtOoby/view?usp=sharing">Presentation
                        with links</a> &
                    <a href="https://huggingface.co/gaussalgo/mt5-large-priming-QA_en-cs">HuggingFace models</a>.</p>
                <img alt="MLMU Cover Photo"
                     src="assets/img/mlmu.jpeg" loading="eager"
                     class="rounded-full object-cover" style="width: 560px; height: 315px;">
                <br></br>
            </div>


            <h3 id="dadc_results" class="mb-0">Mitigating Biases of QA Models by Simple Resampling Methods</h3>
            <div class="subheading mb-3"><span class="text-primary">2022 Annual Conference of the NAACL: DADC Workshop, July 2022</span>
                <br></br>
                <p>Results announcement and recording of Supersamplers' team presentation, live on NAACL 2022
                    DADC workshop in Seattle.</p>
                <iframe width="560" height="315" src="https://www.youtube.com/embed/lMochKhgdZg"
                        title="YouTube video player" frameborder="0"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                        allowfullscreen></iframe>
                <br></br>
            </div>


            <h3 id="srw_talk" class="mb-0">Methods for Estimating and Improving Robustness of Language Models</h3>
            <div class="subheading mb-3"><span
                    class="text-primary">NAACL 2022: SRW workshop, July 2022</span>
                <br></br>
                <p>A 5min talk covering my thesis proposal presented on NAACL 2022 SRW in Seattle.</p>
                <iframe width="560" height="315" src="https://www.youtube.com/embed/dTltkOXeeyk"
                        title="YouTube video player" frameborder="0"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                        allowfullscreen></iframe>
                <br></br>
            </div>

            <h3 id="adaptor_talk" class="mb-0">Adaptor: Objective-Centric Adaptation Library for Language Models</h3>
            <div class="subheading mb-3"><span class="text-primary">60th Annual Meeting of the ACL, May 2022</span>

                <br></br>
                <p>Blitz 2min introduction video of Adaptor library presented on ACL 2022 in Dublin.</p>
                <iframe width="560" height="315" src="https://www.youtube.com/embed/soP6gIVr46E"
                        title="YouTube video player" frameborder="0"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                        allowfullscreen></iframe>
                <br></br>
            </div>
        </div>

    </section>
    <hr class="m-0"/>

    <!-- Other -->
    <section class="resume-section" id="other">
        <div class="resume-section-content">
            <h2 class="mb-5">Other Activities</h2>

            <div class="d-flex flex-column flex-md-row justify-content-between">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Lecturer</h3>
                    <div class="subheading mb-3">Course: Introduction to Information Retrieval</div>
                    <p>Over the five years, I have given practicals for <b>100+ MSc students</b> on the essentials of
                        text representations, transformers and machine learning applied in indexing and search.</p>
                    <div>I have also proposed and led the <b>redesign</b> of the course to an extracurricular
                        competition
                        based on a newly-created, easy-to-use
                        <a href="https://github.com/MIR-MU/pv211-utils">benchmark framework</a>.
                        End-of-semester surveys report that after the introduction of this competition, <b>31.5%</b> of
                        more
                        students agree with the statement that "the course has an educational value and enriches me"
                        compared to the previous years.
                        The results of the competition also allow us to easily identify and reach out
                        to the best students with further research opportunities in the lab.

                    </div>

                    <br></br>
                </div>

                <div class="flex-shrink-0"><span class="text-primary">Springs 2019 - now</span></div>
            </div>
            <div class="d-flex flex-column flex-md-row justify-content-between">
                <div class="flex-grow-1">
                    <h3 class="mb-0" id="supervisor">Supervisor</h3>
                    <div class="subheading mb-3">Transformers Club & others</div>
                    <p>I have supervised numerous bachelor's and master's theses on topics closely related
                        to the robustness of language models. For the more ambitious students, I organize the weekly
                        meetings of <strong>TransformersClub‚Ñ¢</strong>, a shared platform for pursuing creative
                        ideas, peer reviewing and organizing teams to address more complex problems. Many of these
                        initiatives end up as a research paper in major ML/NLP venues.</p>
                    <p>The theses that emerged from the Club or I have supervised:</p>
                    <ul>
                        <li><strong>Marek Kadlƒç√≠k</strong> (2022-2024): <a href="https://is.muni.cz/th/vd9wm/?lang=en">Improving
                            Arithmetical Reasoning of Language Models</a>:
                            <a href="https://aclanthology.org/2023.emnlp-main.742">üìÑ EMNLP-Main paper</a>,
                            <a href="https://openreview.net/forum?id=zBh79GuLNO">üìÑ ICLR LLMAgents paper</a>,
                            üèÜ a <a href="https://www.fi.muni.cz/about/awards/awards.html.en">Dean's award</a>.
                        </li>
                        <li><strong>≈†√°rka ≈†ƒçavnick√°</strong> (2022-2024): <a
                                href="https://nlp.fi.muni.cz/raslan/2022/paper17.pdf">Document Understanding through
                            Visual Question Answering</a>.
                        </li>
                        <li><strong>D√°vid Melu≈°</strong> (2022-2024): <a href="https://is.muni.cz/th/o88qt/?lang=en">Enhancing
                            Quality of Optical Character Recognition</a> with specialized language models.
                        </li>
                        <li><strong>Luk√°≈° Mikula</strong> (2021-2023): <a href="https://is.muni.cz/th/adh58/?lang=en">Think
                            Twice Before You Answer:
                            Mitigating Biases of Question Answering Models</a>;
                            <a href="https://aclanthology.org/2024.eacl-long.133">üìÑ EACL-Main paper</a>,
                            ü•á 1st place in <a href="https://dadcworkshop.github.io/">NAACL DADC</a> shared task Track 2,
                            üèÜ a <a href="https://www.fi.muni.cz/about/awards/awards.html.en">Dean's award</a>.
                        </li>
                        <li><strong>Marek Petroviƒç</strong> (2021-2022): <a href="https://is.muni.cz/th/htvm4/?lang=en">One
                            Bit at a Time: Impact of
                            Quantisation on Neural Machine Translation</a>;
                            ü•â 3rd place in <a href="http://math.sk/svoc2022/#vysledky">SVOC</a> competition,
                            üèÜ a <a href="https://www.fi.muni.cz/about/awards/awards.html.en">Dean's award</a>.
                        </li>
                        <li><strong>Matej Me≈°ko</strong> (2022): <a href="https://is.muni.cz/th/utq47/?lang=en">Evaluation
                            of a Supervised
                            Approach to Information Retrieval</a>
                        </li>
                        <li><strong>Martin Geletka</strong> (2020-2021): <a href="https://is.muni.cz/th/if8vh/?lang=en">Speeding
                            up inference time of
                            neural machine translation</a>;
                            üéñ Nomination for a Dean's award.
                        </li>
                        <li><strong>Petr Miƒçka</strong> (2021): <a href="https://is.muni.cz/th/z52vo/?lang=en">Utilisation
                            of language
                            representations for Information Retrieval</a>
                        </li>

                    </ul>
                    <br></br>
                </div>
                <div class="flex-shrink-0"><span class="text-primary">Autumn 2020 - now</span></div>
            </div>

            <div class="d-flex flex-column flex-md-row justify-content-between">
                <div class="flex-grow-1">
                    <h3 class="mb-0">Reviewer</h3>
                    <div class="subheading mb-3">*ACL Conferences, ACL Rolling Review & other</div>
                    <p>I give back to the community by volunteering for reviewing, proofreading and providing feedback
                        to other researchers whenever it can help. Feel free to get in touch if you're interested!
                        <br></br>
                        <br></br>
                </div>

            </div>

        </div>
    </section>

    <!-- Other -->
    <section class="resume-section" id="afk">
        <div class="resume-section-content">
            <h2 class="mb-5">Away From Keyboard</h2>
            <br></br>
            <br>Research can be fun, but there's a lot of other good stuff to do.
            Whenever I can, I take my shoes ü•æ, a bike üö≤ or skis ‚õ∑ and head towards mountains ‚õ∞‚òÄÔ∏è.
            I love spending time breathing fresh air with unlimited open space ‚òÅ above my head.
            These (and any other) activities feel best when combined with a taste of a filtered brew ‚òï.
            </br><br>Please get in touch if your coffee is undrinkable sour for you. I'll be interested to know your
            source.</br>
        </div>


    </section>
</div>
</section>
<hr class="m-0"/>
</div>
<!-- Bootstrap core JS-->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
<!-- Core theme JS-->
<script src="js/scripts.js"></script>
</body>
</html>

